{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DLMarines","text":"<p>Team: Bartosz Brzoza, Magdalena Buszka, Martyna Firgolska</p> <p>Description: Image recognition of marine animals.</p> <p>Running: To run all pipelines follow instructions from instalation guide and use command:</p> <pre><code>kedro run\n</code></pre>"},{"location":"callbacks/","title":"Callbacks","text":""},{"location":"callbacks/#src.dlmarines.pipelines.model_training.callbacks.LogPredictionSamplesCallback","title":"<code>LogPredictionSamplesCallback</code>","text":"<p>         Bases: <code>pl.Callback</code></p> <p>Callback class for logging. Inherits from pytorch lightning Callback</p> Source code in <code>src/dlmarines/pipelines/model_training/callbacks.py</code> <pre><code>class LogPredictionSamplesCallback(pl.Callback):\n\"\"\"Callback class for logging. Inherits from pytorch lightning Callback\"\"\"\n    def __init__(self, id_to_class):\n\"\"\"Initialize logging Callback\n\n        Args:\n            id_to_class (dict): maps ids of classes to their names\n        \"\"\"\n        super().__init__()\n        self.id_to_class = id_to_class\n        self.accuracies = []\n\n    def on_validation_batch_end(\n        self, trainer, pl_module, outputs, batch, batch_idx\n    ):\n\"\"\"Called when the validation batch ends.\\n\n        Logs accurancy for each batch.\\n\n        Logs sample images with their classifications for a single batch in epoch.\"\"\"\n        x, y = batch\n        y_preds = outputs[1].argmax(-1)\n        if batch_idx == 0:\n            n = 16\n\n            probabilities = outputs[1][np.arange(len(y)), y]\n\n            images = [img for img in x[:n]]\n            captions = [\n                f'Ground Truth: {self.id_to_class[y_i.item()]} Prediction: {self.id_to_class[y_pred.item()]} Certainty: {prob.item():.2%}' \n                for y_i, y_pred, prob in zip(y[:n], y_preds[:n], probabilities[:n])]\n\n\n            trainer.logger.log_image(\n                key='sample_images', \n                images=images, \n                caption=captions\n            )\n        self.accuracies.append((y_preds==y).float().mean().item())\n\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n\"\"\"Called when validation epoch ends.\\n\n        Logs validation accuracy\"\"\"\n        super().on_validation_epoch_end(trainer, pl_module)\n        pl_module.log('val_acc', np.mean(self.accuracies))\n        self.accuracies = []\n</code></pre>"},{"location":"callbacks/#src.dlmarines.pipelines.model_training.callbacks.LogPredictionSamplesCallback.__init__","title":"<code>__init__(id_to_class)</code>","text":"<p>Initialize logging Callback</p> <p>Parameters:</p> Name Type Description Default <code>id_to_class</code> <code>dict</code> <p>maps ids of classes to their names</p> required Source code in <code>src/dlmarines/pipelines/model_training/callbacks.py</code> <pre><code>def __init__(self, id_to_class):\n\"\"\"Initialize logging Callback\n\n    Args:\n        id_to_class (dict): maps ids of classes to their names\n    \"\"\"\n    super().__init__()\n    self.id_to_class = id_to_class\n    self.accuracies = []\n</code></pre>"},{"location":"callbacks/#src.dlmarines.pipelines.model_training.callbacks.LogPredictionSamplesCallback.on_validation_batch_end","title":"<code>on_validation_batch_end(trainer, pl_module, outputs, batch, batch_idx)</code>","text":"<p>Called when the validation batch ends.</p> <p>Logs accurancy for each batch.</p> <p>Logs sample images with their classifications for a single batch in epoch.</p> Source code in <code>src/dlmarines/pipelines/model_training/callbacks.py</code> <pre><code>def on_validation_batch_end(\n    self, trainer, pl_module, outputs, batch, batch_idx\n):\n\"\"\"Called when the validation batch ends.\\n\n    Logs accurancy for each batch.\\n\n    Logs sample images with their classifications for a single batch in epoch.\"\"\"\n    x, y = batch\n    y_preds = outputs[1].argmax(-1)\n    if batch_idx == 0:\n        n = 16\n\n        probabilities = outputs[1][np.arange(len(y)), y]\n\n        images = [img for img in x[:n]]\n        captions = [\n            f'Ground Truth: {self.id_to_class[y_i.item()]} Prediction: {self.id_to_class[y_pred.item()]} Certainty: {prob.item():.2%}' \n            for y_i, y_pred, prob in zip(y[:n], y_preds[:n], probabilities[:n])]\n\n\n        trainer.logger.log_image(\n            key='sample_images', \n            images=images, \n            caption=captions\n        )\n    self.accuracies.append((y_preds==y).float().mean().item())\n</code></pre>"},{"location":"callbacks/#src.dlmarines.pipelines.model_training.callbacks.LogPredictionSamplesCallback.on_validation_epoch_end","title":"<code>on_validation_epoch_end(trainer, pl_module)</code>","text":"<p>Called when validation epoch ends.</p> <p>Logs validation accuracy</p> Source code in <code>src/dlmarines/pipelines/model_training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer, pl_module):\n\"\"\"Called when validation epoch ends.\\n\n    Logs validation accuracy\"\"\"\n    super().on_validation_epoch_end(trainer, pl_module)\n    pl_module.log('val_acc', np.mean(self.accuracies))\n    self.accuracies = []\n</code></pre>"},{"location":"data_downloading/","title":"data downloading","text":""},{"location":"data_downloading/#data-downloading","title":"Data downloading","text":"<p>You can download data manually from https://www.kaggle.com/datasets/vencerlanz09/sea-animals-image-dataste into <code>data/01_raw</code> or use data_downloading pipeline by running</p> <pre><code>kedro run\u00a0--pipeline=data_downloading\n</code></pre> <p>Note that the pipeline uses kaggle api, so you have to download kaggle Api Key if you want to run it. You can find how to do it on instalation section</p>"},{"location":"data_downloading/#overview","title":"Overview","text":"<p>This pipeline connects to kaggle API using the user credentials stored in credentials file and downloads the zip containing the dataset</p>"},{"location":"data_downloading/#pipeline-inputs","title":"Pipeline inputs","text":"<ul> <li>Kaggle credentials of the user (instruction for downloading the credentials can be found in main README</li> </ul>"},{"location":"data_downloading/#src.dlmarines.pipelines.data_downloading.nodes.download_dataset","title":"<code>download_dataset(kaggle_credentials)</code>","text":"<p>Downloads sea animals image dataset via kaggle api.</p> <p>Parameters:</p> Name Type Description Default <code>kaggle_credentials</code> <code>dict</code> <p>Kaggle credentials. Has to contain fields:</p> <ul> <li>'KAGGLE_USERNAME' (string) - kaggle username</li> <li>'KAGGLE_KEY' (string) - kaggle api key</li> </ul> required Source code in <code>src/dlmarines/pipelines/data_downloading/nodes.py</code> <pre><code>def download_dataset(kaggle_credentials: dict):\n\"\"\"Downloads sea animals image dataset via kaggle api.\n\n    Args:\n        kaggle_credentials (dict): Kaggle credentials. Has to contain fields:\\n\n            - 'KAGGLE_USERNAME' (string) - kaggle username\n            - 'KAGGLE_KEY' (string) - kaggle api key\n    \"\"\"\n    _setup_evironment_variables(kaggle_credentials)\n    api = _connect_to_kaggle()\n    # typo in link is intentional\n    api.dataset_download_files('vencerlanz09/sea-animals-image-dataste', path=\"./data/01_raw\")\n</code></pre>"},{"location":"data_processing/","title":"data processing","text":""},{"location":"data_processing/#data-preprocesing","title":"Data preprocesing","text":"<p>To preprocess data from <code>/data/01_raw/sea-animals-image-dataste.zip</code> use data_processing pipeline</p> <pre><code>kedro run --pipeline=data_processing\n</code></pre>"},{"location":"data_processing/#overview","title":"Overview","text":"<p>This pipeline processes the data. First the raw data is unzipped, then loaded using PartitionedDateset and finally it resized and turned into tensors </p>"},{"location":"data_processing/#pipeline-outputs","title":"Pipeline outputs","text":"<ul> <li>processed Dataset - dataset containg processed images, saved as pickle</li> </ul>"},{"location":"data_processing/#src.dlmarines.pipelines.data_processing.nodes.load_dataset","title":"<code>load_dataset(unzipped)</code>","text":"<p>Loades dataset as ImageDataSet.</p> <p>Parameters:</p> Name Type Description Default <code>unzipped</code> <code>any</code> <p>indicates that the previous node has been run</p> required <p>Returns:</p> Name Type Description <code>PartitionedDataSet</code> <p>dataset loaded from \"data/01_raw/\"</p> Source code in <code>src/dlmarines/pipelines/data_processing/nodes.py</code> <pre><code>def load_dataset(unzipped):\n\"\"\"Loades dataset as ImageDataSet.\n\n    Args:\n        unzipped (any): indicates that the previous node has been run\n\n    Returns:\n        PartitionedDataSet: dataset loaded from \"data/01_raw/\"\n    \"\"\"\n    dataset = PartitionedDataSet(\n       path=\"data/01_raw/\",\n       dataset=\"pillow.ImageDataSet\",\n       filename_suffix=\".jpg\"\n    ).load()\n    return dataset\n</code></pre>"},{"location":"data_processing/#src.dlmarines.pipelines.data_processing.nodes.preprocess_dataset","title":"<code>preprocess_dataset(dataset)</code>","text":"<p>Preprocesses dataset. Each image in dataset is resized and transformed to tensor.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>PartitionedDataSet</code> <p>Loaded image dataset</p> required <p>Returns:</p> Name Type Description <code>PartitionedDataSet</code> <p>transformed dataset</p> Source code in <code>src/dlmarines/pipelines/data_processing/nodes.py</code> <pre><code>def preprocess_dataset(dataset):\n\"\"\"Preprocesses dataset. Each image in dataset is resized and transformed to tensor.\n\n    Args:\n        dataset (PartitionedDataSet): Loaded image dataset\n\n    Returns:\n        PartitionedDataSet: transformed dataset\n    \"\"\"\n    transform = torchvision.transforms.Compose([\n        # TODO: add size to params\n        torchvision.transforms.Resize((128, 128)),\n        torchvision.transforms.ToTensor(),\n    ])\n    transformed_dataset = dataset.copy()\n\n    for k, v in tqdm(dataset.items()):\n        transformed_dataset[k] = transform(v())\n    return transformed_dataset\n</code></pre>"},{"location":"data_processing/#src.dlmarines.pipelines.data_processing.nodes.unzip_dataset","title":"<code>unzip_dataset()</code>","text":"<p>Unzips the dataset.</p> <p>Note that for this function to work correctly the sea animals dataset  should be already downloaded into <code>data/01_raw/sea-animals-image-dataste.zip</code>  either manually or via running data_downloading pipeline.</p> <p>Returns:</p> Name Type Description <code>string</code> <p>'unzipped'</p> Source code in <code>src/dlmarines/pipelines/data_processing/nodes.py</code> <pre><code>def unzip_dataset():\n\"\"\"Unzips the dataset.\n\n    Note that for this function to work correctly the sea animals dataset \n    should be already downloaded into `data/01_raw/sea-animals-image-dataste.zip` \n    either manually or via running data_downloading pipeline.\n\n    Returns:\n        string: 'unzipped'\n    \"\"\"\n    with ZipFile('data/01_raw/sea-animals-image-dataste.zip', 'r') as f:\n        f.extractall('data/01_raw')\n    # TODO: return path to unzipped dataset\n    return 'unzipped'\n</code></pre>"},{"location":"datamodule/","title":"MarinesDataModule","text":""},{"location":"datamodule/#src.dlmarines.pipelines.model_training.datamodule.MarinesDataModule","title":"<code>MarinesDataModule</code>","text":"<p>         Bases: <code>pl.LightningDataModule</code></p> <p>Marine Data Module class. Inherits from pytorch LightningDataModule</p> Source code in <code>src/dlmarines/pipelines/model_training/datamodule.py</code> <pre><code>class MarinesDataModule(pl.LightningDataModule):\n\"\"\"Marine Data Module class.\n    Inherits from pytorch LightningDataModule\n    \"\"\"\n    def __init__(self, dataset, batch_size=32, num_workers=8):\n\"\"\"Initializes data module.\n        Prepares labels for data, and transforms tensors of pictures to have 3 channels.\n\n        Args:\n            dataset (Dataset): dataset \n            batch_size (int, optional): Size of batch. Defaults to 32.\n            num_workers (int, optional): Number of workers. Defaults to 8.\n        \"\"\"\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        class_to_id = defaultdict(lambda: len(class_to_id))\n        dataset_new = []\n        dataset = dict(dataset)\n        for k, v in dataset.items():\n            sample_class = k.split('/')[0]\n            sample_class_id = class_to_id[sample_class]\n            if v.shape[0] == 1:\n                v = torch.vstack([v, v, v])\n            elif v.shape[0] == 4:\n                v = v[:3,:,:]\n            dataset_new.append((v, sample_class_id))\n        self.dataset = dataset_new\n        self.id_to_class = {v: k for k, v in class_to_id.items()}\n        self.split = False\n\n    def split_dataset(self, fraction=0.2, seed=42):\n\"\"\"Splits the dataset into train, validation and test\n\n        Args:\n            fraction (float, optional): Fraction of dataset used for testing and validation. Defaults to 0.2.\n            seed (int, optional): Random seed used for splitting. Defaults to 42.\n        \"\"\"\n        n = len(self.dataset)\n        k = int(fraction*n)\n        self.train, self.val, self.test = random_split(\n            self.dataset, [n-2*k, k, k],\n            generator=torch.Generator().manual_seed(seed)\n        )\n        self.split = True\n\n    def setup(self, stage):\n\"\"\"Performs setup\"\"\"\n        if not self.split:\n            self.split_dataset()\n\n    def train_dataloader(self):\n\"\"\"Dataloader for training\"\"\"\n        return DataLoader(self.train, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self):\n\"\"\"Dataloader for validation\"\"\"\n        return DataLoader(self.val, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def test_dataloader(self):\n\"\"\"Dataloader for testing\"\"\"\n        return DataLoader(self.test, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"datamodule/#src.dlmarines.pipelines.model_training.datamodule.MarinesDataModule.__init__","title":"<code>__init__(dataset, batch_size=32, num_workers=8)</code>","text":"<p>Initializes data module. Prepares labels for data, and transforms tensors of pictures to have 3 channels.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>dataset </p> required <code>batch_size</code> <code>int</code> <p>Size of batch. Defaults to 32.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>Number of workers. Defaults to 8.</p> <code>8</code> Source code in <code>src/dlmarines/pipelines/model_training/datamodule.py</code> <pre><code>def __init__(self, dataset, batch_size=32, num_workers=8):\n\"\"\"Initializes data module.\n    Prepares labels for data, and transforms tensors of pictures to have 3 channels.\n\n    Args:\n        dataset (Dataset): dataset \n        batch_size (int, optional): Size of batch. Defaults to 32.\n        num_workers (int, optional): Number of workers. Defaults to 8.\n    \"\"\"\n    super().__init__()\n    self.batch_size = batch_size\n    self.num_workers = num_workers\n    class_to_id = defaultdict(lambda: len(class_to_id))\n    dataset_new = []\n    dataset = dict(dataset)\n    for k, v in dataset.items():\n        sample_class = k.split('/')[0]\n        sample_class_id = class_to_id[sample_class]\n        if v.shape[0] == 1:\n            v = torch.vstack([v, v, v])\n        elif v.shape[0] == 4:\n            v = v[:3,:,:]\n        dataset_new.append((v, sample_class_id))\n    self.dataset = dataset_new\n    self.id_to_class = {v: k for k, v in class_to_id.items()}\n    self.split = False\n</code></pre>"},{"location":"datamodule/#src.dlmarines.pipelines.model_training.datamodule.MarinesDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Performs setup</p> Source code in <code>src/dlmarines/pipelines/model_training/datamodule.py</code> <pre><code>def setup(self, stage):\n\"\"\"Performs setup\"\"\"\n    if not self.split:\n        self.split_dataset()\n</code></pre>"},{"location":"datamodule/#src.dlmarines.pipelines.model_training.datamodule.MarinesDataModule.split_dataset","title":"<code>split_dataset(fraction=0.2, seed=42)</code>","text":"<p>Splits the dataset into train, validation and test</p> <p>Parameters:</p> Name Type Description Default <code>fraction</code> <code>float</code> <p>Fraction of dataset used for testing and validation. Defaults to 0.2.</p> <code>0.2</code> <code>seed</code> <code>int</code> <p>Random seed used for splitting. Defaults to 42.</p> <code>42</code> Source code in <code>src/dlmarines/pipelines/model_training/datamodule.py</code> <pre><code>def split_dataset(self, fraction=0.2, seed=42):\n\"\"\"Splits the dataset into train, validation and test\n\n    Args:\n        fraction (float, optional): Fraction of dataset used for testing and validation. Defaults to 0.2.\n        seed (int, optional): Random seed used for splitting. Defaults to 42.\n    \"\"\"\n    n = len(self.dataset)\n    k = int(fraction*n)\n    self.train, self.val, self.test = random_split(\n        self.dataset, [n-2*k, k, k],\n        generator=torch.Generator().manual_seed(seed)\n    )\n    self.split = True\n</code></pre>"},{"location":"datamodule/#src.dlmarines.pipelines.model_training.datamodule.MarinesDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Dataloader for testing</p> Source code in <code>src/dlmarines/pipelines/model_training/datamodule.py</code> <pre><code>def test_dataloader(self):\n\"\"\"Dataloader for testing\"\"\"\n    return DataLoader(self.test, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"datamodule/#src.dlmarines.pipelines.model_training.datamodule.MarinesDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Dataloader for training</p> Source code in <code>src/dlmarines/pipelines/model_training/datamodule.py</code> <pre><code>def train_dataloader(self):\n\"\"\"Dataloader for training\"\"\"\n    return DataLoader(self.train, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"datamodule/#src.dlmarines.pipelines.model_training.datamodule.MarinesDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Dataloader for validation</p> Source code in <code>src/dlmarines/pipelines/model_training/datamodule.py</code> <pre><code>def val_dataloader(self):\n\"\"\"Dataloader for validation\"\"\"\n    return DataLoader(self.val, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"instalation/","title":"Installation","text":""},{"location":"instalation/#installation","title":"Installation:","text":"<p>To install this project make sure you have conda and poetry installed. After that simply open terminal, go to project folder and type:  </p> <pre><code>conda env create  --file conda.yml\nconda activate dlmarines\npoetry install\n</code></pre> <p>Note that the download pipeline uses kaggle api, so in order to run it follow the steps below to download your kaggle key.</p> <p>Download Kaggle Api Key: 1. Sign in to kaggle 2. Go to Account 3. Go to API section and click <code>Create New API Token</code>. It will download <code>kaggle.json</code> with your username and key.</p> <pre><code>{ \"username\":\"your_kaggle_username\",\"key\":\"123456789\"}\n</code></pre> <ol> <li>In <code>conf\\local\\credentials.yml</code> add your username and key as shown below:</li> </ol> <pre><code>kaggle:\n      username: \"your_kaggle_username\"\n      key: \"123456789\"\n</code></pre>"},{"location":"model/","title":"MarineModel","text":""},{"location":"model/#src.dlmarines.pipelines.model_training.model.MarineModel","title":"<code>MarineModel</code>","text":"<p>         Bases: <code>pl.LightningModule</code></p> <p>MarineModel class. Inherits from pytorch LightningModule</p> Source code in <code>src/dlmarines/pipelines/model_training/model.py</code> <pre><code>class MarineModel(pl.LightningModule):\n\"\"\"MarineModel class. Inherits from pytorch LightningModule\n    \"\"\"\n    def __init__(self, params):\n\"\"\"Initializes model\n\n        Args:\n            params (dict): Dict of parameters described below\n            params.cnn_channles(int): number of cnn channels for convolutional layer\n            params.kernel_size(int): kernel size in convolutional layer\n            params.num_cnn_layers: number of convolutional layers\n            params.flattened_size: number of input neurons for first fully connected layer\n            params.fc_features: number of neurons in fully connected layers\n            params.num_fc_layers: number of fully conected layers\n            params.num_classes: number of classes to be classified\n        \"\"\"\n        super().__init__()\n        self.params = params\n        layers = []\n        layers.append(\n            nn.Conv2d(3, params['cnn_channels'], params['kernel_size'])\n        )\n        layers.append(nn.ReLU())\n        for _ in range(params['num_cnn_layers']-1):\n            layers.append(nn.Conv2d(params['cnn_channels'], params['cnn_channels'], params['kernel_size']))\n            layers.append(nn.ReLU())\n        layers.append(nn.Flatten())\n        layers.append(nn.Linear(params['flattened_size'], params['fc_features']))\n        layers.append(nn.ReLU())\n        for _ in range(params['num_fc_layers']-1):\n            layers.append(nn.Linear(params['fc_features'], params['fc_features']))\n            layers.append(nn.ReLU())\n        layers.append(nn.Linear(params['fc_features'], params['num_classes']))\n        layers.append(nn.Softmax())\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log(\"val_loss\", loss)\n        return loss, y_hat\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.params['learning_rate'])\n</code></pre>"},{"location":"model/#src.dlmarines.pipelines.model_training.model.MarineModel.__init__","title":"<code>__init__(params)</code>","text":"<p>Initializes model</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Dict of parameters described below</p> required <code>params.cnn_channles(int)</code> <p>number of cnn channels for convolutional layer</p> required <code>params.kernel_size(int)</code> <p>kernel size in convolutional layer</p> required <code>params.num_cnn_layers</code> <p>number of convolutional layers</p> required <code>params.flattened_size</code> <p>number of input neurons for first fully connected layer</p> required <code>params.fc_features</code> <p>number of neurons in fully connected layers</p> required <code>params.num_fc_layers</code> <p>number of fully conected layers</p> required <code>params.num_classes</code> <p>number of classes to be classified</p> required Source code in <code>src/dlmarines/pipelines/model_training/model.py</code> <pre><code>def __init__(self, params):\n\"\"\"Initializes model\n\n    Args:\n        params (dict): Dict of parameters described below\n        params.cnn_channles(int): number of cnn channels for convolutional layer\n        params.kernel_size(int): kernel size in convolutional layer\n        params.num_cnn_layers: number of convolutional layers\n        params.flattened_size: number of input neurons for first fully connected layer\n        params.fc_features: number of neurons in fully connected layers\n        params.num_fc_layers: number of fully conected layers\n        params.num_classes: number of classes to be classified\n    \"\"\"\n    super().__init__()\n    self.params = params\n    layers = []\n    layers.append(\n        nn.Conv2d(3, params['cnn_channels'], params['kernel_size'])\n    )\n    layers.append(nn.ReLU())\n    for _ in range(params['num_cnn_layers']-1):\n        layers.append(nn.Conv2d(params['cnn_channels'], params['cnn_channels'], params['kernel_size']))\n        layers.append(nn.ReLU())\n    layers.append(nn.Flatten())\n    layers.append(nn.Linear(params['flattened_size'], params['fc_features']))\n    layers.append(nn.ReLU())\n    for _ in range(params['num_fc_layers']-1):\n        layers.append(nn.Linear(params['fc_features'], params['fc_features']))\n        layers.append(nn.ReLU())\n    layers.append(nn.Linear(params['fc_features'], params['num_classes']))\n    layers.append(nn.Softmax())\n    self.model = nn.Sequential(*layers)\n</code></pre>"},{"location":"model_evaluation/","title":"model evaluation","text":""},{"location":"model_evaluation/#overview","title":"Overview","text":"<p>This pipeline evaluates the model. It creates the Test Logger and DataModule, to make Test Trainer. It reads Trained Model saved by model_tarining pipeline and utilizes Test Triner to test it.  </p>"},{"location":"model_evaluation/#pipeline-inputs","title":"Pipeline inputs","text":"<ul> <li>PreprocessedDataset - dataset outputed by data_processing pipeline</li> <li>Training parameters -  parameters defining the training (for logging)</li> <li>logger parameters - parameters defining the wndb project for which to log</li> </ul>"},{"location":"model_evaluation/#pipeline-outputs","title":"Pipeline outputs","text":"<ul> <li>Tested model - the model after testing</li> </ul>"},{"location":"model_evaluation/#src.dlmarines.pipelines.model_evaluation.nodes.test_model","title":"<code>test_model(model, trainer, datamodule)</code>","text":"<p>Tests the model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MarineModel</code> <p>trained model.</p> required <code>trainer</code> <code>Trainer</code> <p>trainer used for testing.</p> required <code>datamodule</code> <code>MarinesDataModule</code> <p>datamodule with data for testing.</p> required <p>Returns:</p> Name Type Description <code>MarineModel</code> <p>tested model</p> Source code in <code>src/dlmarines/pipelines/model_evaluation/nodes.py</code> <pre><code>def test_model(model, trainer, datamodule):\n\"\"\"Tests the model\n\n    Args:\n        model (MarineModel): trained model.\n        trainer (Trainer): trainer used for testing.\n        datamodule (MarinesDataModule): datamodule with data for testing.\n\n    Returns:\n        MarineModel: tested model\n    \"\"\"\n    trainer.test(\n        model,\n        datamodule=datamodule\n    )\n    return model\n</code></pre>"},{"location":"model_training/","title":"model training","text":""},{"location":"model_training/#model-training","title":"Model training","text":"<p>To train the model use model_training pipeline</p> <pre><code>kedro run --pipeline=model_training\n</code></pre>"},{"location":"model_training/#overview","title":"Overview","text":"<p>This pipeline creates DataModule form PreprocessedDataset, as well as creates Logger and Model from parameters defined in model_training.yml. It then makes the Trainer using Datamodlue and Logger, after which it uses it to train the model. The train-test split is performed inside the DataModule.</p>"},{"location":"model_training/#pipeline-inputs","title":"Pipeline inputs","text":"<ul> <li>PreprocessedDataset - dataset outputed by data_processing pipeline</li> <li>Training parameters -  parameters defining the training</li> <li>model parameters - parameters defining the structure of the model</li> <li>logger parameters - parameters defining the wndb project for which to log</li> </ul>"},{"location":"model_training/#pipeline-outputs","title":"Pipeline outputs","text":"<ul> <li>trained model - model after trianing, </li> </ul>"},{"location":"model_training/#src.dlmarines.pipelines.model_training.nodes.create_model","title":"<code>create_model(params)</code>","text":"<p>Create Marine Model</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>parameters for MarineModel</p> required <p>Returns:</p> Name Type Description <code>MarineModel</code> <p>untrained model</p> Source code in <code>src/dlmarines/pipelines/model_training/nodes.py</code> <pre><code>def create_model(params):\n\"\"\"Create Marine Model\n\n    Args:\n        params (dict): parameters for MarineModel\n\n    Returns:\n        MarineModel: untrained model\n    \"\"\"\n    model = MarineModel(params)\n    return model\n</code></pre>"},{"location":"model_training/#src.dlmarines.pipelines.model_training.nodes.get_datamodule","title":"<code>get_datamodule(dataset)</code>","text":"<p>Get marine datamodule for given dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>PartitionedDataSet</code> <p>dataset to be transformed into datamodule</p> required <p>Returns:</p> Name Type Description <code>MarinesDataModule</code> <p>datamodule with marine dataset data</p> Source code in <code>src/dlmarines/pipelines/model_training/nodes.py</code> <pre><code>def get_datamodule(dataset):\n\"\"\"Get marine datamodule for given dataset\n\n    Args:\n        dataset (PartitionedDataSet): dataset to be transformed into datamodule\n\n    Returns:\n        MarinesDataModule: datamodule with marine dataset data\n    \"\"\"\n    datamodule = MarinesDataModule(dataset)\n    return datamodule\n</code></pre>"},{"location":"model_training/#src.dlmarines.pipelines.model_training.nodes.get_logger","title":"<code>get_logger(logger_params, model_params)</code>","text":"<p>Returns WandB logger</p> <p>Parameters:</p> Name Type Description Default <code>logger_params</code> <code>dict</code> <p>parameters for logger.  (Contains 'project_name' and 'entity_name' for WandB logger)</p> required <code>model_params</code> <code>_type_</code> <p>parameters for model being trained. These parameters are loged so that different configurations can be compared.</p> required <p>Returns:</p> Name Type Description <code>WandbLogger</code> <p>logger</p> Source code in <code>src/dlmarines/pipelines/model_training/nodes.py</code> <pre><code>def get_logger(logger_params, model_params):\n\"\"\"Returns WandB logger\n\n    Args:\n        logger_params (dict): parameters for logger. \n            (Contains 'project_name' and 'entity_name' for WandB logger)\n        model_params (_type_): parameters for model being trained.\n            These parameters are loged so that different configurations can be compared.\n\n    Returns:\n        WandbLogger: logger\n    \"\"\"\n    logger = pl.loggers.WandbLogger(\n        project=logger_params['project_name'],\n        entity=logger_params['entity_name'],\n        config=model_params,\n        log_model=True,\n    )\n    return logger\n</code></pre>"},{"location":"model_training/#src.dlmarines.pipelines.model_training.nodes.get_trainer","title":"<code>get_trainer(logger, datamodule, params)</code>","text":"<p>Get the trainer</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>logger</p> required <code>datamodule</code> <code>MarinesDataModule</code> <p>datamodule, used to get id to class name mapping</p> required <code>params</code> <code>dict</code> <p>additional parameters for trainer (eg. num_epochs)</p> required <p>Returns:</p> Name Type Description <code>Trainer</code> <p>pytorch Lighting trainer</p> Source code in <code>src/dlmarines/pipelines/model_training/nodes.py</code> <pre><code>def get_trainer(logger, datamodule, params):\n\"\"\"Get the trainer\n\n    Args:\n        logger (Logger): logger\n        datamodule (MarinesDataModule): datamodule, used to get id to class name mapping\n        params (dict): additional parameters for trainer (eg. num_epochs)\n\n    Returns:\n        Trainer: pytorch Lighting trainer\n    \"\"\"\n    trainer = pl.Trainer(\n        max_epochs=params['num_epochs'],\n        # TODO: add lots of params\n        logger=logger,\n        callbacks=[\n            LogPredictionSamplesCallback(datamodule.id_to_class),\n        ]\n    )\n    return trainer\n</code></pre>"},{"location":"model_training/#src.dlmarines.pipelines.model_training.nodes.train_model","title":"<code>train_model(model, trainer, datamodule)</code>","text":"<p>Trains the model using provided trainer and datamodule</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MarineModel</code> <p>model to be trained</p> required <code>trainer</code> <code>Trainer</code> <p>pytorch trainer to be used</p> required <code>datamodule</code> <code>MarinesDataModule</code> <p>datamodule with data for training</p> required <p>Returns:</p> Name Type Description <code>MarineModel</code> <p>trained model</p> Source code in <code>src/dlmarines/pipelines/model_training/nodes.py</code> <pre><code>def train_model(model, trainer, datamodule):\n\"\"\"Trains the model using provided trainer and datamodule\n\n    Args:\n        model (MarineModel): model to be trained\n        trainer (Trainer): pytorch trainer to be used\n        datamodule (MarinesDataModule): datamodule with data for training\n\n    Returns:\n        MarineModel: trained model\n    \"\"\"\n    trainer.fit(\n        model,\n        datamodule=datamodule\n    )\n    return model\n</code></pre>"}]}